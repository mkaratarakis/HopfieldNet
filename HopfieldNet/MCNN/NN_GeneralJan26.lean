import Mathlib

open scoped InnerProductSpace RealInnerProductSpace

-- We work over a general Nontrivially Normed Field ùïú.
variable {ùïú : Type*} [NontriviallyNormedField ùïú]

namespace ContinuousLinearMap

variable {E F G : Type*}
  [NormedAddCommGroup E] [NormedSpace ùïú E]
  [NormedAddCommGroup F] [NormedSpace ùïú F]
  [NormedAddCommGroup G] [NormedSpace ùïú G]

/-- The continuous linear map that composes a continuous linear map with a given continuous linear
map `f` on the right. This is the "right-composition" operator.
`compRightL ùïú E F` is the map `g ‚Ü¶ g.comp f` where `f : E ‚ÜíL[ùïú] F` and `g : F ‚ÜíL[ùïú] G`. -/
noncomputable def compRightL (f : E ‚ÜíL[ùïú] F) : (F ‚ÜíL[ùïú] G) ‚ÜíL[ùïú] (E ‚ÜíL[ùïú] G) :=
  (ContinuousLinearMap.compL ùïú E F G).flip f

@[simp]
theorem compRightL_apply (f : E ‚ÜíL[ùïú] F) (g : F ‚ÜíL[ùïú] G) :
    compRightL f g = g.comp f :=
  rfl

/-- The dual map of a continuous linear map `f`, is the continuous linear map from the dual of the
codomain to the dual of the domain, given by pre-composition with `f`. -/
noncomputable def dualMap (f : E ‚ÜíL[ùïú] F) :
    StrongDual ùïú F ‚ÜíL[ùïú] StrongDual ùïú E :=
  compRightL f

@[simp]
theorem dualMap_apply {f : E ‚ÜíL[ùïú] F} {g : StrongDual ùïú F} :
    dualMap f g = g.comp f := rfl

@[simp]
theorem dualMap_apply_apply {f : E ‚ÜíL[ùïú] F} {g : StrongDual ùïú F} {x : E} :
    (dualMap f g) x = g (f x) := rfl

@[simp]
theorem dualMap_comp {f : E ‚ÜíL[ùïú] F} {g : F ‚ÜíL[ùïú] G} :
    dualMap (g.comp f) = (dualMap f).comp (dualMap g) := by
  ext h
  simp only [comp_apply, dualMap_apply, ContinuousLinearMap.comp_assoc]

end ContinuousLinearMap

/-!
# L1 Generalized: Differentiable Pullbacks (Banach Spaces)
-/

-- E, F, G are Normed Spaces over ùïú (Banach if CompleteSpace is assumed).
variable {E F G : Type*}
  [NormedAddCommGroup E] [NormedSpace ùïú E]
  [NormedAddCommGroup F] [NormedSpace ùïú F]
  [NormedAddCommGroup G] [NormedSpace ùïú G]

/--
The fundamental abstraction for differentiable computation in Banach spaces.
Represents a function and its backpropagator operating on the dual spaces (the pullback).
-/
structure DifferentiablePullback (E F : Type*) [NormedAddCommGroup E] [NormedSpace ùïú E]
  [NormedAddCommGroup F] [NormedSpace ùïú F] where
  view : E ‚Üí F
  h_diff : Differentiable ùïú view
  /-- The pullback: E ‚Üí (F* ‚ÜíL[ùïú] E*). Returns the dual map of Df(x). -/
  pullback : E ‚Üí (StrongDual ùïú F ‚ÜíL[ùïú] StrongDual ùïú E)
  /-- Correctness: The pullback map must be the dual map of the Fr√©chet derivative. -/
  h_pullback : ‚àÄ (x : E),
    pullback x = ContinuousLinearMap.dualMap (fderiv ùïú view x)

namespace DifferentiablePullback

def compose {E F G : Type*} [NormedAddCommGroup E] [NormedSpace ùïú E]
    [NormedAddCommGroup F] [NormedSpace ùïú F] [NormedAddCommGroup G] [NormedSpace ùïú G]
    (L1 : @DifferentiablePullback ùïú _ F G _ _ _ _) (L2 : @DifferentiablePullback ùïú _ E F _ _ _ _) :
    @DifferentiablePullback ùïú _ E G _ _ _ _ where
  view := L1.view ‚àò L2.view
  h_diff := L1.h_diff.comp L2.h_diff
  pullback := fun x =>
    -- (g ‚àò f)* = f* ‚àò g*
    (L2.pullback x).comp (L1.pullback (L2.view x))
  h_pullback := by
    intro x
    simp only [L2.h_pullback x, L1.h_pullback (L2.view x)]
    -- Now we have: goal is to show
    -- (L2.pullback x).comp (L1.pullback (L2.view x)) =
    -- dualMap (fderiv ùïú (L1.view ‚àò L2.view) x)
    rw [‚Üê ContinuousLinearMap.dualMap_comp]
    -- Now we need to show:
    -- dualMap ((fderiv ùïú L1.view (L2.view x)).comp (fderiv ùïú L2.view x)) =
    -- dualMap (fderiv ùïú (L1.view ‚àò L2.view) x)
    congr 1
    rw [‚Üê fderiv_comp x (L1.h_diff (L2.view x)) (L2.h_diff x)]

end DifferentiablePullback

/-!
# L1 Specialization: Differentiable Lenses (Hilbert Spaces)
-/

-- H1, H2, H3 are Hilbert spaces. We require ùïú' to be RCLike (‚Ñù or ‚ÑÇ) for the standard Hilbert adjoint.
variable {ùïú' : Type*} [RCLike ùïú']
variable {H1 H2 H3 : Type*}
  [NormedAddCommGroup H1] [InnerProductSpace ùïú' H1] [CompleteSpace H1]
  [NormedAddCommGroup H2] [InnerProductSpace ùïú' H2] [CompleteSpace H2]
  [NormedAddCommGroup H3] [InnerProductSpace ùïú' H3] [CompleteSpace H3]

/--
A Differentiable Lens in Hilbert spaces. Uses the Hilbert adjoint for gradient flow.
-/
structure DifferentiableLens (ùïú' : Type*) (H1 H2 : Type*)
  [RCLike ùïú']
  [NormedAddCommGroup H1] [InnerProductSpace ùïú' H1] [CompleteSpace H1]
  [NormedAddCommGroup H2] [InnerProductSpace ùïú' H2] [CompleteSpace H2] where
  view : H1 ‚Üí H2
  h_diff : Differentiable ùïú' view
  /-- The backward map (Adjoint): H1 ‚Üí (H2 ‚ÜíL[ùïú'] H1). -/
  update : H1 ‚Üí (H2 ‚ÜíL[ùïú'] H1)
  /-- Correctness: The update map must be the Hilbert adjoint of the Fr√©chet derivative. -/
  h_update : ‚àÄ (x : H1),
    update x = ContinuousLinearMap.adjoint (fderiv ùïú' view x)

namespace DifferentiableLens

/-- Composition of Lenses (The Chain Rule in Hilbert Spaces). -/
def compose {ùïú' : Type*} [RCLike ùïú'] {H1 H2 H3 : Type*}
    [NormedAddCommGroup H1] [InnerProductSpace ùïú' H1] [CompleteSpace H1]
    [NormedAddCommGroup H2] [InnerProductSpace ùïú' H2] [CompleteSpace H2]
    [NormedAddCommGroup H3] [InnerProductSpace ùïú' H3] [CompleteSpace H3]
    (L1 : DifferentiableLens ùïú' H1 H2) (L2 : DifferentiableLens ùïú' H2 H3) :
    DifferentiableLens ùïú' H1 H3 where
  view := L2.view ‚àò L1.view
  h_diff := L2.h_diff.comp L1.h_diff
  update := fun x =>
    let y := L1.view x
    -- (g ‚àò f)‚Ä† = f‚Ä† ‚àò g‚Ä†
    (L1.update x).comp (L2.update y)
  h_update := by
    intro x
    simp_rw [L1.h_update, L2.h_update (L1.view x)]
    -- Apply the chain rule: fderiv (g ‚àò f) x = (fderiv g (f x)) ‚àò (fderiv f x)
    rw [fderiv_comp x (L2.h_diff (L1.view x)) (L1.h_diff x)]
    rw [ContinuousLinearMap.adjoint_comp]

end DifferentiableLens

/-!
# L1 Refinement: Higher-Order Calculus
-/

section HigherOrderCalculus

variable {ùïú : Type*} [NontriviallyNormedField ùïú]
variable {E F : Type*} [NormedAddCommGroup E] [NormedSpace ùïú E] [NormedAddCommGroup F] [NormedSpace ùïú F]

/--
The second derivative (Hessian) of a function f: E ‚Üí F at x.
It is the Fr√©chet derivative of the Fr√©chet derivative map.
H(x) : E ‚ÜíL[ùïú] (E ‚ÜíL[ùïú] F).
-/
noncomputable def Hessian (f : E ‚Üí F) (x : E) : E ‚ÜíL[ùïú] (E ‚ÜíL[ùïú] F) :=
  fderiv (ùïú := ùïú) (fderiv (ùïú := ùïú) f) x

/--
Hessian-Vector Products (Hv-products). Computes (H(x)v‚ÇÅv‚ÇÇ).
This is the second derivative evaluated in the directions v‚ÇÅ and v‚ÇÇ.
-/
noncomputable def HessianVectorProduct (f : E ‚Üí F) (x v‚ÇÅ v‚ÇÇ : E) : F :=
  ((Hessian (ùïú := ùïú) (E := E) (F := F) f x) v‚ÇÅ) v‚ÇÇ

-- Note: Higher-order derivatives are accessed directly via `iteratedFDeriv ùïú n f x`.

-- H is a Hilbert space over ‚Ñù for optimization contexts (required for `gradient`).
variable {H : Type*} [NormedAddCommGroup H] [InnerProductSpace ‚Ñù H] [CompleteSpace H]

/--
Hessian-Vector Products (Hv-products) for scalar-valued functions. Computes H(x)v.
This utilizes the definition that the Hessian applied to v is the directional derivative
of the gradient along v (Forward-over-Reverse AD).
The result is a vector in H.
-/
noncomputable def HessianVectorProduct' (f : H ‚Üí ‚Ñù) (x v : H) : H :=
  let g := gradient f
  (fderiv ‚Ñù g x) v

end HigherOrderCalculus

/-!
# L1 Refinement: The Riesz Bridge
We formalize the connection between the Banach dual map and the Hilbert adjoint.
-/

section RieszBridge

-- H1, H2 are Hilbert spaces over ùïú' (‚Ñù or ‚ÑÇ).
variable {ùïú' : Type*} [RCLike ùïú']
variable {H1 H2 : Type*}
  [NormedAddCommGroup H1] [InnerProductSpace ùïú' H1] [CompleteSpace H1]
  [NormedAddCommGroup H2] [InnerProductSpace ùïú' H2] [CompleteSpace H2]

/--
The Riesz Representation Map H ‚âÉL[ùïú'] H*.
It is a conjugate-linear isometric isomorphism targeting the `StrongDual`.
In mathlib: `InnerProductSpace.toDual`.
-/
noncomputable abbrev RieszMap (H : Type*) [NormedAddCommGroup H] [InnerProductSpace ùïú' H] [CompleteSpace H] :
  H ‚âÉ‚Çó·µ¢‚ãÜ[ùïú'] StrongDual ùïú' H :=
  InnerProductSpace.toDual ùïú' H

/--
Theorem: The Riesz Bridge.
The Hilbert adjoint L‚Ä† is related to the Banach dual map L* by the Riesz isomorphisms R_H:
L‚Ä† = R‚ÇÅ‚Åª¬π ‚àò L* ‚àò R‚ÇÇ.
This shows that the optimization geometry (Hilbert adjoint) is derived from
the differentiation mechanism (Banach dual map).
-/
theorem riesz_bridge_adjoint
    (L : H1 ‚ÜíL[ùïú'] H2) :
    L.adjoint =
      ((RieszMap H1).symm.toContinuousLinearEquiv.toContinuousLinearMap).comp
        ((ContinuousLinearMap.dualMap L).comp
        ((RieszMap H2).toContinuousLinearEquiv.toContinuousLinearMap)) := by
  simp; exact rfl

end RieszBridge

/-!
# L2: Parameterized Lenses for Neural Networks
-/

section ParameterizedLens

variable {ùïú' : Type*} [RCLike ùïú']
variable {P H_in H_out : Type*}
  [NormedAddCommGroup P] [InnerProductSpace ùïú' P] [CompleteSpace P]
  [NormedAddCommGroup H_in] [InnerProductSpace ùïú' H_in] [CompleteSpace H_in]
  [NormedAddCommGroup H_out] [InnerProductSpace ùïú' H_out] [CompleteSpace H_out]

-- Provide an inner product space instance for the Unit type.
noncomputable instance : InnerProductSpace ùïú' Unit where
  inner _ _ := 0
  norm_sq_eq_re_inner _ := by simp
  conj_inner_symm _ _ := by simp
  add_left _ _ _ := by simp
  smul_left _ _ _ := by simp

/-
/--
A DifferentiableLens that is parameterized by a set of weights `P`.
The `view` function now takes parameters `p` and an input `x`.
The `update` function computes the adjoint, which can be used to derive gradients
with respect to both the parameters and the input.
-/
structure ParameterizedLens (ùïú' : Type*) (P H_in H_out : Type*)
  [RCLike ùïú']
  [NormedAddCommGroup P] [InnerProductSpace ùïú' P] [CompleteSpace P]
  [NormedAddCommGroup H_in] [InnerProductSpace ùïú' H_in] [CompleteSpace H_in]
  [NormedAddCommGroup H_out] [InnerProductSpace ùïú' H_out] [CompleteSpace H_out] where
  view : P ‚Üí H_in ‚Üí H_out
  h_diff : Differentiable ùïú' (fun (ph : P √ó H_in) => view ph.1 ph.2)
  /-- The backward map (Adjoint): P √ó H_in ‚Üí (H_out ‚ÜíL[ùïú'] P √ó H_in). -/
  update : P ‚Üí H_in ‚Üí (H_out ‚ÜíL[ùïú'] P √ó H_in)
  /-- Correctness: The update map must be the Hilbert adjoint of the Fr√©chet derivative. -/
  h_update : ‚àÄ (p : P) (x : H_in),
    update p x = ContinuousLinearMap.adjoint (fderiv ùïú' (fun (ph : P √ó H_in) => view ph.1 ph.2) (p, x))

namespace ParameterizedLens

/--
An affine layer `x ‚Ü¶ Ax + b`.
Parameters `P` are `(H_in ‚ÜíL[ùïú'] H_out) √ó H_out`, representing the matrix `A` and bias `b`.
We require `H_in` to be finite-dimensional to have an inner product on the space of linear maps.
-/
def affineLayer (H_in H_out : Type*) [NormedAddCommGroup H_in] [InnerProductSpace ùïú' H_in] [CompleteSpace H_in] [FiniteDimensional ùïú' H_in]
    [NormedAddCommGroup H_out] [InnerProductSpace ùïú' H_out] [CompleteSpace H_out] :
    ParameterizedLens ùïú' ((H_in ‚ÜíL[ùïú'] H_out) √ó H_out) H_in H_out where
  view p x := p.1 x + p.2
  h_diff := by
    let f1 : ((H_in ‚ÜíL[ùïú'] H_out) √ó H_out) √ó H_in ‚Üí (H_in ‚ÜíL[ùïú'] H_out) √ó H_in := fun p_x => (p_x.1.1, p_x.2)
    let f2 : (H_in ‚ÜíL[ùïú'] H_out) √ó H_in ‚Üí H_out := fun p_x => p_x.1 p_x.2
    let f3 : ((H_in ‚ÜíL[ùïú'] H_out) √ó H_out) √ó H_in ‚Üí H_out := fun p_x => p_x.1.2
    have h_f1 : Differentiable ùïú' f1 := by simp; exact differentiable_fst.prod differentiable_snd
    have h_f2 : Differentiable ùïú' f2 := isBoundedBilinearMap_apply.differentiable
    have h_f3 : Differentiable ùïú' f3 := by simp; exact differentiable_snd.comp differentiable_fst
    exact (h_f2.comp h_f1).add h_f3
  update p x := ContinuousLinearMap.adjoint (fderiv ùïú' (fun ph => view ph.1 ph.2) (p, x))
  h_update p x := rfl

/--
An element-wise activation layer, e.g., ReLU or sigmoid.
It has no parameters (`P = Unit`), so it's a special case of a `ParameterizedLens`.
-/
def elementwise (f : H_in ‚Üí H_out) (h_f : Differentiable ùïú' f) :
    ParameterizedLens ùïú' Unit H_in H_out where
  view _ x := f x
  h_diff := Differentiable.comp h_f differentiable_snd
  update _ x := ContinuousLinearMap.adjoint (fderiv ùïú' (fun (ph : Unit √ó H_in) => f ph.2) ((), x))
  h_update _ _ := rfl

/--
Mean Squared Error loss function: `L(y_pred, y_true) = ‚Äñy_pred - y_true‚Äñ¬≤`.
This is a `ParameterizedLens` with `H_in = H_out √ó H_out` (predicted and true values)
and output `H_out = ‚Ñù`. It has no parameters (`P = Unit`).
-/
def mseLoss (H : Type*) [NormedAddCommGroup H] [InnerProductSpace ùïú' H] [CompleteSpace H] :
    ParameterizedLens ùïú' Unit (H √ó H) ‚Ñù where
  view _ y_yh := ‚Äñy_yh.1 - y_yh.2‚Äñ ^ 2
  h_diff := by
    have h_norm_sq : Differentiable ùïú' (fun (v : H) => ‚Äñv‚Äñ ^ 2) := by
      simp_rw [‚Üê inner_self_eq_norm_sq_to_K]
      have := isBoundedBilinearMap_inner ùïú' H
      exact this.differentiable.comp (differentiable_id.prod_mk differentiable_id)
    exact Differentiable.comp h_norm_sq (differentiable_fst.sub differentiable_snd)
  update _ y_yh := ContinuousLinearMap.adjoint (fderiv ùïú' (fun (ph : Unit √ó (H √ó H)) => ‚Äñph.2.1 - ph.2.2‚Äñ ^ 2) ((), y_yh))
  h_update _ _ := rfl

variable {P1 H1 H2 P2 H3 : Type*}
  [NormedAddCommGroup P1] [InnerProductSpace ùïú' P1] [CompleteSpace P1]
  [NormedAddCommGroup H1] [InnerProductSpace ùïú' H1] [CompleteSpace H1]
  [NormedAddCommGroup H2] [InnerProductSpace ùïú' H2] [CompleteSpace H2]
  [NormedAddCommGroup P2] [InnerProductSpace ùïú' P2] [CompleteSpace P2]
  [NormedAddCommGroup H3] [InnerProductSpace ùïú' H3] [CompleteSpace H3]

/-- Composition of ParameterizedLenses. -/
def compose (L2 : ParameterizedLens ùïú' P2 H2 H3) (L1 : ParameterizedLens ùïú' P1 H1 H2) :
    ParameterizedLens ùïú' (P1 √ó P2) H1 H3 where
  view p x := L2.view p.2 (L1.view p.1 x)
  h_diff := by
    let f_combined : (P1 √ó P2) √ó H1 ‚Üí (P2 √ó H2) :=
      fun p_x => (p_x.1.2, L1.view p_x.1.1 p_x.2)
    have h_f_combined : Differentiable ùïú' f_combined :=
      (differentiable_snd.comp differentiable_fst).prod (L1.h_diff.comp ((differentiable_fst.comp differentiable_fst).prod differentiable_snd))
    exact L2.h_diff.comp h_f_combined
  update p x := ContinuousLinearMap.adjoint (fderiv ùïú' (fun ph => view ph.1 ph.2) (p, x))
  h_update p x := rfl

/--
A single step of gradient descent for a parameterized lens.
Computes the gradient of the loss with respect to the parameters and updates them.
-/
def gradientDescentStep
    (L : ParameterizedLens ùïú' P H_in H_out)
    (loss : ParameterizedLens ùïú' Unit (H_out √ó H_out) ‚Ñù)
    (p : P) (x : H_in) (y_true : H_out) (Œ∑ : ‚Ñù) : P :=
  let y_pred := L.view p x
  let _loss_val := loss.view () (y_pred, y_true)
  -- The adjoint of the composed forward map gives the gradients.
  -- The derivative of the loss w.r.t. its input is needed.
  -- For MSE `‚Äñy - y'‚Äñ¬≤`, the gradient w.r.t. `y` is `2(y - y')`.
  -- The `update` function for the loss gives the adjoint of the derivative.
  -- We apply it to `1` (the gradient of the identity function `z ‚Ü¶ z` at `loss_val`).
  let dL_dy_pred_adj : Unit √ó (H_out √ó H_out) := (loss.update () (y_pred, y_true)) (1 : ‚Ñù)
  -- The adjoint returns a pair of gradients: (w.r.t. y_pred, w.r.t. y_true). We need the first.
  let dL_dy_pred := dL_dy_pred_adj.2.1
  -- Propagate this gradient back through the lens L.
  let grads := (L.update p x) dL_dy_pred
  -- The result is a pair of gradients: (w.r.t. parameters, w.r.t. input). We need the first.
  let dL_dp := grads.1
  -- Update parameters
  p - (Œ∑ : ùïú') ‚Ä¢ dL_dp

end ParameterizedLens
-/

/-!
# Differentiable computational blocks and VJPs (general Banach + Hilbert specializations)

A CompBlock is a smooth, parameterized operation fwd : P √ó X ‚Üí Y over a base field ùïú.
We expose its Jacobian (via fderiv), a Banach-space VJP (via the dual map), and a
Hilbert-space VJP (via the adjoint). We also provide a bridge back into your
DifferentiablePullback abstraction for reuse of composition theorems.
-/

section CompBlock

variable {ùïú : Type*} [NontriviallyNormedField ùïú]
variable {P X Y : Type*}
  [NormedAddCommGroup P] [NormedSpace ùïú P]
  [NormedAddCommGroup X] [NormedSpace ùïú X]
  [NormedAddCommGroup Y] [NormedSpace ùïú Y]

/-- A differentiable, parameterized block: fwd : P √ó X ‚Üí Y with a differentiability certificate. -/
structure CompBlock (ùïú : Type*) [NontriviallyNormedField ùïú]
    (P X Y : Type*)
    [NormedAddCommGroup P] [NormedSpace ùïú P]
    [NormedAddCommGroup X] [NormedSpace ùïú X]
    [NormedAddCommGroup Y] [NormedSpace ùïú Y] where
  fwd   : P √ó X ‚Üí Y
  diff  : Differentiable ùïú fwd

namespace CompBlock

/-- The Fr√©chet derivative (Jacobian) of a block at (p, x). -/
noncomputable def jacobian
    (B : CompBlock ùïú P X Y) (p : P) (x : X) : (P √ó X) ‚ÜíL[ùïú] Y :=
  fderiv ùïú B.fwd (p, x)

/-- Banach VJP: the pullback on duals (StrongDual) induced by the Jacobian. -/
noncomputable def vjpBanach
    (B : CompBlock ùïú P X Y) (p : P) (x : X) :
    (StrongDual ùïú Y) ‚ÜíL[ùïú] (StrongDual ùïú (P √ó X)) :=
  ContinuousLinearMap.dualMap (B.jacobian p x)

/-- Package the block as a DifferentiablePullback from P √ó X to Y. -/
noncomputable def toDifferentiablePullback
    (B : CompBlock ùïú P X Y) :
    @DifferentiablePullback ùïú _ (P √ó X) Y _ _ _ _ where
  view := B.fwd
  h_diff := B.diff
  pullback := fun z => ContinuousLinearMap.dualMap (fderiv ùïú B.fwd z)
  h_pullback := by intro z; rfl

/- Hilbert-space VJP via the adjoint (requires inner products over an RCLike field). -/
variable {ùïú' : Type*} [RCLike ùïú']
variable {P' X' Y' : Type*}
  [NormedAddCommGroup P'] [InnerProductSpace ùïú' P'] [CompleteSpace P']
  [NormedAddCommGroup X'] [InnerProductSpace ùïú' X'] [CompleteSpace X']
  [NormedAddCommGroup Y'] [InnerProductSpace ùïú' Y'] [CompleteSpace Y']

open scoped InnerProductSpace RealInnerProductSpace PiLp EuclideanSpace

/-
noncomputable def vjpHilbert
    (B : CompBlock ùïú' P' X' Y') (p : P') (x : X') :
    Y' ‚ÜíL[ùïú'] (P' √ó X') :=
  have := Prod.innerProductSpace
  ContinuousLinearMap.adjoint (fderiv ùïú' B.fwd (p, x))
  -/



end CompBlock
end CompBlock


open scoped InnerProductSpace RealInnerProductSpace BigOperators Gradient

variable {P S : Type*}
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup S] [InnerProductSpace ‚Ñù S] [CompleteSpace S]

/-- Componentwise inner product on `P √ó S` (sum of inner products). -/
noncomputable instance : Inner ‚Ñù (P √ó S) where
  inner x y := inner ‚Ñù x.1 y.1 + inner ‚Ñù x.2 y.2

/--
  **Definition: EnergyLens**
  A bundled structure connecting Thermodynamics (Energy) with Mechanics (Forces).
  It strictly separates Logic (Prop) from Data (Map).
-/
structure EnergyLens (P S : Type*)
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup S] [InnerProductSpace ‚Ñù S] [CompleteSpace S] where

  /-- The Scalar Potential E(Œ∏, s) -/
  energy : P ‚Üí S ‚Üí ‚Ñù

  /-- The Constructive Force (Data). Returns raw vectors for O(1) execution. -/
  force_vector : P ‚Üí S ‚Üí (P √ó S)

  /-- The Riesz Consistency Certificate (Logic). -/
  is_gradient : ‚àÄ p s (v : P √ó S),
    inner ‚Ñù (force_vector p s) v =
      fderiv ‚Ñù (fun (x : P √ó S) => energy x.1 x.2) (p, s) v

/--
Construct a force vector from an energy function by taking the Fr√©chet derivative
and using the Riesz map (`InnerProductSpace.toDual`) on each factor.

This is the canonical way to get a vector-valued gradient without having to prove
any differentiability facts about `energy` (since `fderiv` is defined with a default
value when not differentiable).
-/
noncomputable def forceFromEnergy (energy : P ‚Üí S ‚Üí ‚Ñù) : P ‚Üí S ‚Üí (P √ó S) :=
  fun p s =>
    let f : P √ó S ‚Üí ‚Ñù := fun ps => energy ps.1 ps.2
    let df : (P √ó S) ‚ÜíL[‚Ñù] ‚Ñù := fderiv ‚Ñù f (p, s)
    let dfp : P ‚ÜíL[‚Ñù] ‚Ñù := df.comp (ContinuousLinearMap.inl ‚Ñù P S)
    let dfs : S ‚ÜíL[‚Ñù] ‚Ñù := df.comp (ContinuousLinearMap.inr ‚Ñù P S)
    ((InnerProductSpace.toDual ‚Ñù P).symm dfp, (InnerProductSpace.toDual ‚Ñù S).symm dfs)

lemma inner_forceFromEnergy_eq_fderiv
    (energy : P ‚Üí S ‚Üí ‚Ñù) (p : P) (s : S) (v : P √ó S) :
    inner ‚Ñù (forceFromEnergy (P:=P) (S:=S) energy p s) v
      = fderiv ‚Ñù (fun ps : P √ó S => energy ps.1 ps.2) (p, s) v := by
  classical
  let f : P √ó S ‚Üí ‚Ñù := fun ps => energy ps.1 ps.2
  let df : (P √ó S) ‚ÜíL[‚Ñù] ‚Ñù := fderiv ‚Ñù f (p, s)
  have h1 :
      inner ‚Ñù ((InnerProductSpace.toDual ‚Ñù P).symm (df.comp (ContinuousLinearMap.inl ‚Ñù P S))) v.1
        = (df.comp (ContinuousLinearMap.inl ‚Ñù P S)) v.1 := by
    -- Riesz: ‚ü™(toDual.symm y), x‚ü´ = y x
    simp
  have h2 :
      inner ‚Ñù ((InnerProductSpace.toDual ‚Ñù S).symm (df.comp (ContinuousLinearMap.inr ‚Ñù P S))) v.2
        = (df.comp (ContinuousLinearMap.inr ‚Ñù P S)) v.2 := by
    simp
  have hv : v = ContinuousLinearMap.inl ‚Ñù P S v.1 + ContinuousLinearMap.inr ‚Ñù P S v.2 := by
    ext <;> simp [ContinuousLinearMap.inl, ContinuousLinearMap.inr]
  -- unfold forceFromEnergy and compute using linearity of `df`
  calc
    inner ‚Ñù (forceFromEnergy (P:=P) (S:=S) energy p s) v
        =
        inner ‚Ñù ((InnerProductSpace.toDual ‚Ñù P).symm (df.comp (ContinuousLinearMap.inl ‚Ñù P S))) v.1
          +
        inner ‚Ñù ((InnerProductSpace.toDual ‚Ñù S).symm (df.comp (ContinuousLinearMap.inr ‚Ñù P S))) v.2 := by
          -- definitional reduction of `inner` on the product + unfolding `forceFromEnergy`
          dsimp [forceFromEnergy, f, df]; rfl
    _ =
        (df.comp (ContinuousLinearMap.inl ‚Ñù P S)) v.1
          +
        (df.comp (ContinuousLinearMap.inr ‚Ñù P S)) v.2 := by
          simp [h1, h2]
    _ =
        df (ContinuousLinearMap.inl ‚Ñù P S v.1)
          +
        df (ContinuousLinearMap.inr ‚Ñù P S v.2) := by
          rfl
    _ = df (ContinuousLinearMap.inl ‚Ñù P S v.1 + ContinuousLinearMap.inr ‚Ñù P S v.2) := by
          simpa using
            (df.map_add (ContinuousLinearMap.inl ‚Ñù P S v.1)
              (ContinuousLinearMap.inr ‚Ñù P S v.2)).symm
    _ = df v := by grind
    _ = fderiv ‚Ñù (fun ps : P √ó S => energy ps.1 ps.2) (p, s) v := by
          simp [df, f]

-- FIX: Variable declaration moved OUTSIDE the structure to fix scope errors
variable {n : Type*} [Fintype n] [DecidableEq n]

/-! ### 3.1 The Isomorphism (Flatten-and-Lift) -/

/--
  Helper: `EuclideanSpace` is a type wrapper around functions.
  We need this to lift raw functions (like matrix results) into the Analysis type.
-/
noncomputable abbrev toEuclideanFun {Œπ : Type*} [Fintype Œπ] (f : Œπ ‚Üí ‚Ñù) : EuclideanSpace ‚Ñù Œπ :=
  (WithLp.equiv 2 (Œπ ‚Üí ‚Ñù)).symm f

/-- Reshapes a flattened Euclidean vector into a Matrix -/
noncomputable def toMatrix (v : EuclideanSpace ‚Ñù (n √ó n)) : Matrix n n ‚Ñù :=
  Matrix.of (fun i j => v (i, j))

/-- Reshapes a Matrix into a flattened Euclidean vector -/
noncomputable def toFlat (m : Matrix n n ‚Ñù) : EuclideanSpace ‚Ñù (n √ó n) :=
  toEuclideanFun (fun p => m p.1 p.2)

/--
  Bridge: Converts a standard computational block (e.g., a Feedforward Network)
  into an EnergyLens by defining the energy as the squared error loss.
  This allows standard NN architectures to be trained via the Hamiltonian/Langevin dynamics defined below.
-/
noncomputable def toEnergyLens {P X : Type*}
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup X] [InnerProductSpace ‚Ñù X] [CompleteSpace X]
  (B : CompBlock ‚Ñù P X ‚Ñù) (target : ‚Ñù) : EnergyLens P X where
  energy := fun p x => 1/2 * ‚ÄñB.fwd (p, x) - target‚Äñ^2
  force_vector := fun p x =>
    let y := B.fwd (p, x)
    let err := y - target
    -- The gradient of 1/2‚Äñy - t‚Äñ¬≤ is (y - t).
    -- We backpropagate (y - t) through the block.
    -- Note: This assumes we have a Hilbert VJP available (adjoint).
    -- For now, we leave the implementation abstract or use a placeholder.
    ((0 : P), (0 : X)) -- Placeholder: Requires Hilbert VJP implementation
  is_gradient := sorry

/-! ### 3.2 The Physics (Mean-Field Theory) -/

/-- Binary Entropy: Forces the continuous state to act like a discrete bit -/
noncomputable def binary_entropy (x : ‚Ñù) : ‚Ñù :=
  x * Real.log x + (1 - x) * Real.log (1 - x)

/--
  **Entropic Boltzmann Machine**
  Params: EuclideanSpace (n √ó n) (Weights) √ó EuclideanSpace n (Bias)
  State:  EuclideanSpace n
-/
noncomputable def EntropicBoltzmannEnergy
    (p : WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n)) (x : EuclideanSpace ‚Ñù n) : ‚Ñù :=
  let W_flat := p.1.1
  let Œ∏ := p.1.2
  let W := toMatrix W_flat
  let Wx := toEuclideanFun (W.mulVec x)
  (-0.5 : ‚Ñù) * inner ‚Ñù x Wx - inner ‚Ñù Œ∏ x + (‚àë i, binary_entropy (x i))

/--
An explicit, hand-computed force/gradient for `EntropicBoltzmannEnergy`.

This is useful for documentation and fast computation.

Note: proving this equals the canonical `forceFromEnergy` construction (and hence satisfies
`EnergyLens.is_gradient`) requires extra analytic hypotheses and lemmas, especially for the
entropy term (you typically need `‚àÄ i, 0 < x i ‚àß x i < 1` to use `Real.log` derivative rules).
-/
noncomputable def EntropicBoltzmannForceExplicit
    (p : WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n)) (x : EuclideanSpace ‚Ñù n) :
    WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n) √ó EuclideanSpace ‚Ñù n :=
  let W_flat := p.1.1
  let Œ∏ := p.1.2
  let W := toMatrix W_flat

  -- 1. Dynamics (State Force): ‚àá_x E = -0.5 (W + W·µÄ) x - Œ∏ + logit(x)
  let Wx := toEuclideanFun (W.mulVec x)
  let WTx := toEuclideanFun (W.transpose.mulVec x)
  let dE_dx_lin := (-0.5 : ‚Ñù) ‚Ä¢ (Wx + WTx) - Œ∏
  let dE_dx_ent := toEuclideanFun (fun i => Real.log (x i / (1 - x i)))
  let dE_dx := dE_dx_lin + dE_dx_ent

  -- 2. Learning (Parameter Force): ‚àá_W E = -¬Ω(x ‚äó x),  ‚àá_Œ∏ E = -x
  let dE_dW_matrix : Matrix n n ‚Ñù := (-0.5 : ‚Ñù) ‚Ä¢ (Matrix.vecMulVec x x)
  let dE_dW_flat := toFlat dE_dW_matrix
  let dE_dŒ∏ := -x

  (WithLp.toLp 2 (dE_dW_flat, dE_dŒ∏), dE_dx)

noncomputable def EntropicBoltzmann :
    EnergyLens (WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n)) (EuclideanSpace ‚Ñù n) :=
{
  -- Energy = Interaction + Confinement + Entropy
  energy := EntropicBoltzmannEnergy (n := n)

  -- Force vector defined canonically from the Fr√©chet derivative (Riesz).
  force_vector := forceFromEnergy (P:=WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n))
    (S:=EuclideanSpace ‚Ñù n) (EntropicBoltzmannEnergy (n := n))

  is_gradient := by
    intro p x v
    -- `force_vector` was defined as `forceFromEnergy`, so the gradient identity is automatic.
    simpa [EntropicBoltzmannEnergy] using
      (inner_forceFromEnergy_eq_fderiv
        (P:=WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n))
        (S:=EuclideanSpace ‚Ñù n)
        (energy := EntropicBoltzmannEnergy (n := n))
        p x v)
}

/--
The (nontrivial) analytic statement needed to justify that the explicit formula
`EntropicBoltzmannForceExplicit` is the Riesz/Fr√©chet gradient of `EntropicBoltzmannEnergy` at `(p, x)`.

Concretely, it asserts that the partial Fr√©chet derivatives in the parameter and state directions
are represented by inner products with the explicit gradients.

Proving this in full requires assumptions like `‚àÄ i, 0 < x i ‚àß x i < 1` (to use derivative rules
for `Real.log` in the entropy term) plus standard differentiability lemmas for the quadratic form.
-/
def EntropicBoltzmannForceExplicitCorrect
    (p : WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n)) (x : EuclideanSpace ‚Ñù n) : Prop :=
  let f :
      (WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n)) √ó (EuclideanSpace ‚Ñù n) ‚Üí ‚Ñù :=
    fun px => EntropicBoltzmannEnergy (n := n) px.1 px.2
  let df :
      ((WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n)) √ó (EuclideanSpace ‚Ñù n)) ‚ÜíL[‚Ñù] ‚Ñù :=
    fderiv ‚Ñù f (p, x)
  df.comp (ContinuousLinearMap.inl ‚Ñù _ _) =
      (InnerProductSpace.toDual ‚Ñù (WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n)))
        (EntropicBoltzmannForceExplicit (n := n) p x).1
    ‚àß
    df.comp (ContinuousLinearMap.inr ‚Ñù _ _) =
      (InnerProductSpace.toDual ‚Ñù (EuclideanSpace ‚Ñù n))
        (EntropicBoltzmannForceExplicit (n := n) p x).2

omit [DecidableEq n] in
/-- Under `EntropicBoltzmannForceExplicitCorrect`, the explicit force matches the canonical force. -/
theorem EntropicBoltzmannForceExplicit_eq_force_vector
    (p : WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n)) (x : EuclideanSpace ‚Ñù n)
    (h : EntropicBoltzmannForceExplicitCorrect (n := n) p x) :
    EntropicBoltzmannForceExplicit (n := n) p x = EntropicBoltzmann.force_vector p x := by
  classical
  -- unpack the correctness hypothesis
  dsimp [EntropicBoltzmannForceExplicitCorrect] at h
  rcases h with ‚ü®hP, hS‚ü©
  -- turn the `toDual` equalities into equalities in the primal spaces
  have hP' :
      (InnerProductSpace.toDual ‚Ñù (WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n))).symm
          ((fderiv ‚Ñù
              (fun px :
                (WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n)) √ó (EuclideanSpace ‚Ñù n) =>
                EntropicBoltzmannEnergy (n := n) px.1 px.2) (p, x)).comp
            (ContinuousLinearMap.inl ‚Ñù _ _)) =
        (EntropicBoltzmannForceExplicit (n := n) p x).1 := by
    have := congrArg
      (fun y =>
        (InnerProductSpace.toDual ‚Ñù (WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n))).symm y) hP
    simpa using this
  have hS' :
      (InnerProductSpace.toDual ‚Ñù (EuclideanSpace ‚Ñù n)).symm
          ((fderiv ‚Ñù
              (fun px :
                (WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n)) √ó (EuclideanSpace ‚Ñù n) =>
                EntropicBoltzmannEnergy (n := n) px.1 px.2) (p, x)).comp
            (ContinuousLinearMap.inr ‚Ñù _ _)) =
        (EntropicBoltzmannForceExplicit (n := n) p x).2 := by
    have := congrArg (fun y => (InnerProductSpace.toDual ‚Ñù (EuclideanSpace ‚Ñù n)).symm y) hS
    simpa using this
  -- compare the two forces by unfolding the canonical one (`forceFromEnergy`)
  refine Prod.ext ?_ ?_
  ¬∑ -- parameter component
    simpa [EntropicBoltzmann, forceFromEnergy, EntropicBoltzmannEnergy] using hP'.symm
  ¬∑ -- state component
    simpa [EntropicBoltzmann, forceFromEnergy, EntropicBoltzmannEnergy] using hS'.symm

namespace EntropicBoltzmann

/-- Alias: explicit force/gradient for documentation/computation. -/
noncomputable abbrev force_vector_explicit
    (p : WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n)) (x : EuclideanSpace ‚Ñù n) :
    WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n) √ó EuclideanSpace ‚Ñù n :=
  EntropicBoltzmannForceExplicit (n := n) p x

/-- Alias: hypothesis packaging the analytic work needed to justify the explicit formula. -/
abbrev force_vector_explicit_correct
    (p : WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n)) (x : EuclideanSpace ‚Ñù n) : Prop :=
  EntropicBoltzmannForceExplicitCorrect (n := n) p x

omit [DecidableEq n] in
/-- If the packaged analytic hypotheses hold, the explicit force equals the canonical `force_vector`. -/
theorem force_vector_explicit_eq_force_vector
    (p : WithLp 2 (EuclideanSpace ‚Ñù (n √ó n) √ó EuclideanSpace ‚Ñù n)) (x : EuclideanSpace ‚Ñù n)
    (h : force_vector_explicit_correct (n := n) p x) :
    force_vector_explicit (n := n) p x = _root_.EntropicBoltzmann.force_vector p x :=
  EntropicBoltzmannForceExplicit_eq_force_vector (n := n) p x h

end EntropicBoltzmann

open Real

noncomputable def neural_sigmoid (x : ‚Ñù) : ‚Ñù := 1 / (1 + exp (-x))

theorem mean_field_consistency
  (W_flat : EuclideanSpace ‚Ñù (n √ó n)) (Œ∏ : EuclideanSpace ‚Ñù n) (s : EuclideanSpace ‚Ñù n) :

  -- If the thermodynamic force is zero (Stationary Point)
  (EntropicBoltzmann.force_vector (WithLp.toLp 2 (W_flat, Œ∏)) s).2 = 0

  -- Then the state satisfies the Discrete Fixed-Point Equation
  -- s = œÉ(Ws + Œ∏)
  ‚Üî ‚àÄ i, s i = neural_sigmoid (((toMatrix W_flat).mulVec s) i + Œ∏ i) :=
by
  -- 1. Force = 0 implies -Wx - Œ∏ + logit(s) = 0
  -- 2. logit(s) = Wx + Œ∏
  -- 3. s = sigmoid(Wx + Œ∏)
  sorry


noncomputable def langevinStep (L : EnergyLens P S)
    (Œ∏ : P) (s : S) (noise : S) (T dt : ‚Ñù) : S :=

  let grad_s := (L.force_vector Œ∏ s).2
  s - (dt ‚Ä¢ grad_s) + (Real.sqrt (2 * T * dt) ‚Ä¢ noise)

noncomputable def contrastiveDivergence (L : EnergyLens P S)
    (Œ∏ : P) (s_pos s_neg : S) (Œ∑ : ‚Ñù) : P :=

    let grad_Œ∏_pos := (L.force_vector Œ∏ s_pos).1
    let grad_Œ∏_neg := (L.force_vector Œ∏ s_neg).1

    Œ∏ - (Œ∑ ‚Ä¢ (grad_Œ∏_pos - grad_Œ∏_neg))

/--
  **Definition: HamiltonianLens**
  Extends `EnergyLens` with a symplectic structure `J` on the phase space `Q √ó M`.
  `Q` represents configuration variables, `M` represents momentum variables.
-/
structure HamiltonianLens (P Q M : Type*)
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup Q] [InnerProductSpace ‚Ñù Q] [CompleteSpace Q]
  [NormedAddCommGroup M] [InnerProductSpace ‚Ñù M] [CompleteSpace M]
  extends EnergyLens P (Q √ó M) where

  /-- Symplectic Linear Map J: PhaseSpace ‚Üí PhaseSpace -/
  J : (Q √ó M) ‚ÜíL[‚Ñù] (Q √ó M)

  /-- J must be skew-adjoint: ‚ü®J u, v‚ü© = -‚ü®u, J v‚ü© -/
  J_skew : ‚àÄ u v, inner ‚Ñù (J u) v = - inner ‚Ñù u (J v)

  /-- Hamiltonian Vector Field: X_H(Œ∏, s) = J ¬∑ ‚àá_s H(Œ∏, s) -/
  hamiltonian_vector_field : P ‚Üí (Q √ó M) ‚Üí (Q √ó M) :=
    fun Œ∏ s => J (force_vector Œ∏ s).2

/--
  **Leapfrog Integrator**
  Explicit symplectic integrator for separable Hamiltonians.
  Performs a half-step in momentum, full step in position, half-step in momentum.
-/
noncomputable def leapfrogStep {P Q M : Type*}
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup Q] [InnerProductSpace ‚Ñù Q] [CompleteSpace Q]
  [NormedAddCommGroup M] [InnerProductSpace ‚Ñù M] [CompleteSpace M]
  (L : HamiltonianLens P Q M) (Œ∏ : P) (s : Q √ó M) (dt : ‚Ñù) : Q √ó M :=
  let (q, m) := s
  -- 1. Half-step Momentum: m_{1/2} = m - (dt/2) * ‚àá_q H(q, m)
  let grad_s_0 := (L.force_vector Œ∏ (q, m)).2
  let grad_q_0 := grad_s_0.1
  let m_half := m - (dt / 2) ‚Ä¢ grad_q_0

  -- 2. Full-step Position: q_{1} = q + dt * ‚àá_m H(q, m_{1/2})
  let grad_s_half := (L.force_vector Œ∏ (q, m_half)).2
  let grad_m_half := grad_s_half.2
  let q_new := q + dt ‚Ä¢ grad_m_half

  -- 3. Half-step Momentum: m_{1} = m_{1/2} - (dt/2) * ‚àá_q H(q_{1}, m_{1/2})
  let grad_s_new := (L.force_vector Œ∏ (q_new, m_half)).2
  let grad_q_new := grad_s_new.1
  let m_new := m_half - (dt / 2) ‚Ä¢ grad_q_new

  (q_new, m_new)

/--
  **Standard Hamiltonian System**
  Constructs a HamiltonianLens from an EnergyLens on phase space Q √ó Q,
  using the canonical symplectic structure J = [[0, I], [-I, 0]].
-/
def StandardHamiltonian {P Q : Type*}
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup Q] [InnerProductSpace ‚Ñù Q] [CompleteSpace Q]
  (L : EnergyLens P (Q √ó Q)) : HamiltonianLens P Q Q :=
  { L with
    J := ContinuousLinearMap.prod
          (ContinuousLinearMap.snd ‚Ñù Q Q)
          (-(ContinuousLinearMap.fst ‚Ñù Q Q))
    J_skew := by
      intros u v
      simp only [ContinuousLinearMap.prod_apply, ContinuousLinearMap.snd_apply,
                 ContinuousLinearMap.neg_apply, ContinuousLinearMap.fst_apply]
      rw [inner_prod_eq_add_inner, inner_prod_eq_add_inner]
      rw [inner_neg_left, inner_neg_right]
      ring
  }

/--
  **Hamiltonian Monte Carlo (HMC)**
  Uses symplectic integration (Leapfrog) to propose a new state in phase space,
  then accepts or rejects based on the energy difference (Metropolis-Hastings).

  Arguments:
  - `L`: The Hamiltonian system (Energy + Symplectic structure).
  - `Œ∏`: Parameters of the energy function.
  - `current_state`: Starting point (q, m) in phase space.
  - `dt`: Time step for the integrator.
  - `n_steps`: Number of leapfrog steps to simulate trajectory.
  - `u`: A uniform random number in [0, 1] for the acceptance check.
-/
noncomputable def HamiltonianMonteCarlo {P Q M : Type*}
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup Q] [InnerProductSpace ‚Ñù Q] [CompleteSpace Q]
  [NormedAddCommGroup M] [InnerProductSpace ‚Ñù M] [CompleteSpace M]
  (L : HamiltonianLens P Q M) (Œ∏ : P) (current_state : Q √ó M)
  (dt : ‚Ñù) (n_steps : ‚Ñï) (u : ‚Ñù) : Q √ó M :=
  let rec trajectory (s : Q √ó M) (n : ‚Ñï) : Q √ó M :=
    match n with
    | 0 => s
    | k + 1 => trajectory (leapfrogStep L Œ∏ s dt) k

  let proposed_state := trajectory current_state n_steps

  let H_old := L.energy Œ∏ current_state
  let H_new := L.energy Œ∏ proposed_state

  if u < Real.exp (H_old - H_new) then
    proposed_state
  else
    current_state

/--
  **Langevin Monte Carlo (MALA)**
  Combines the Langevin diffusion step with a Metropolis-Hastings correction
  to ensure convergence to the exact target distribution.

  Arguments:
  - `L`: EnergyLens defining the potential energy.
  - `Œ∏`: Parameters.
  - `current_state`: Current sample x.
  - `noise`: Standard Gaussian noise vector Œæ.
  - `T`: Temperature.
  - `dt`: Time step size.
  - `u`: Uniform random number [0, 1] for acceptance check.
-/
noncomputable def LangevinMonteCarlo {P S : Type*}
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup S] [InnerProductSpace ‚Ñù S] [CompleteSpace S]
  (L : EnergyLens P S) (Œ∏ : P) (current_state : S) (noise : S)
  (T dt : ‚Ñù) (u : ‚Ñù) : S :=
  -- 1. Propose new state using Langevin dynamics
  let proposed_state := langevinStep L Œ∏ current_state noise T dt

  -- 2. Calculate acceptance probability (Metropolis-Hastings)
  -- Forward transition log-probability (simplified): -||noise||^2 / 2
  let log_q_fwd := - (inner ‚Ñù noise noise) / (2 : ‚Ñù)

  -- Backward transition log-probability
  let grad_prop := (L.force_vector Œ∏ proposed_state).2
  let mean_bwd := proposed_state - (dt ‚Ä¢ grad_prop)
  let dist_bwd := current_state - mean_bwd
  let log_q_bwd := - (inner ‚Ñù dist_bwd dist_bwd) / (4 * T * dt)

  let log_ratio := (L.energy Œ∏ current_state - L.energy Œ∏ proposed_state) / T + log_q_bwd - log_q_fwd

  if Real.log u < log_ratio then proposed_state else current_state

/--
  **Symplectic Map Property**
  A map f is symplectic if it preserves the symplectic form defined by J.
  œâ(u, v) = ‚ü®u, J v‚ü©
  Condition: œâ(df u, df v) = œâ(u, v)
-/
def IsSymplecticMap {E : Type*}
  [NormedAddCommGroup E] [InnerProductSpace ‚Ñù E] [CompleteSpace E]
  (J : E ‚ÜíL[‚Ñù] E) (f : E ‚Üí E) : Prop :=
  ‚àÄ x, DifferentiableAt ‚Ñù f x ‚àß
  ‚àÄ u v,
    inner ‚Ñù (fderiv ‚Ñù f x u) (J (fderiv ‚Ñù f x v)) = inner ‚Ñù u (J v)

/--
  **Linear Shear Transformation**
  S(q, p) = (q, p + Tq) where T is a linear map.
-/
def LinearShear {Q : Type*}
  [NormedAddCommGroup Q] [InnerProductSpace ‚Ñù Q] [CompleteSpace Q]
  (T : Q ‚ÜíL[‚Ñù] Q) : (Q √ó Q) ‚ÜíL[‚Ñù] (Q √ó Q) :=
  ContinuousLinearMap.prod
    (ContinuousLinearMap.fst ‚Ñù Q Q)
    (ContinuousLinearMap.add
      (ContinuousLinearMap.snd ‚Ñù Q Q)
      (T.comp (ContinuousLinearMap.fst ‚Ñù Q Q)))

/--
  **Theorem: Linear Shear is Symplectic**
  Prove that a linear shear transformation is symplectic with respect to the
  standard symplectic structure if the shear operator T is self-adjoint.
-/
theorem linear_shear_is_symplectic {P Q : Type*}
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup Q] [InnerProductSpace ‚Ñù Q] [CompleteSpace Q]
  (L : EnergyLens P (Q √ó Q)) (T : Q ‚ÜíL[‚Ñù] Q) (hT : ContinuousLinearMap.adjoint T = T) :
  IsSymplecticMap (StandardHamiltonian L).J (LinearShear T) := by
  intro x
  constructor
  ¬∑ apply (LinearShear T).differentiableAt
  ¬∑ intro u v
    rw [fderiv_continuousLinearMap]
    obtain ‚ü®q1, p1‚ü© := u
    obtain ‚ü®q2, p2‚ü© := v
    simp only [StandardHamiltonian, LinearShear, ContinuousLinearMap.prod_apply,
               ContinuousLinearMap.fst_apply, ContinuousLinearMap.snd_apply,
               ContinuousLinearMap.add_apply, ContinuousLinearMap.comp_apply,
               ContinuousLinearMap.neg_apply, inner_prod_eq_add_inner,
               inner_add_right, inner_neg_right, inner_add_left]
    rw [‚Üê ContinuousLinearMap.adjoint_inner_left]
    rw [hT]
    ring

/--
  **Verlet Integrator**
  Equivalent to Leapfrog but formulated in terms of positions.
  q_{n+1} = 2q_n - q_{n-1} - dt^2 * ‚àá_q V(q_n)
  Note: This assumes the Hamiltonian is separable H(q,p) = V(q) + |p|^2/2.
-/
noncomputable def VerletIntegrator {P Q : Type*}
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup Q] [InnerProductSpace ‚Ñù Q] [CompleteSpace Q]
  (L : EnergyLens P (Q √ó Q)) (Œ∏ : P) (q_prev q_curr : Q) (dt : ‚Ñù) : Q :=
  let grad_q := (L.force_vector Œ∏ (q_curr, 0)).2.1
  (2 : ‚Ñù) ‚Ä¢ q_curr - q_prev - (dt ^ 2) ‚Ä¢ grad_q

/--
  **Shadow Hamiltonian**
  Represents the modified Hamiltonian $\tilde{H}$ that is exactly conserved by a symplectic integrator.
  For an integrator of order $n$, $\tilde{H} = H + O(\Delta t^n)$.
-/
structure ShadowHamiltonian (P Q M : Type*)
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup Q] [InnerProductSpace ‚Ñù Q] [CompleteSpace Q]
  [NormedAddCommGroup M] [InnerProductSpace ‚Ñù M] [CompleteSpace M]
  extends HamiltonianLens P Q M where

  /-- The original Hamiltonian system -/
  original : HamiltonianLens P Q M

  /-- The time step $\Delta t$ for which this shadow Hamiltonian is defined -/
  dt : ‚Ñù

  /-- The order of accuracy of the integrator -/
  order : ‚Ñï

/--
  **Yoshida Integrator**
  A 4th-order symplectic integrator constructed by composing three 2nd-order Leapfrog steps.
  Coefficients derived by Haruo Yoshida (1990).
-/
noncomputable def YoshidaIntegrator {P Q M : Type*}
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup Q] [InnerProductSpace ‚Ñù Q] [CompleteSpace Q]
  [NormedAddCommGroup M] [InnerProductSpace ‚Ñù M] [CompleteSpace M]
  (L : HamiltonianLens P Q M) (Œ∏ : P) (s : Q √ó M) (dt : ‚Ñù) : Q √ó M :=
  let w1 := 1 / (2 - Real.rpow 2 (1/3 : ‚Ñù))
  let w0 := 1 - 2 * w1
  let s1 := leapfrogStep L Œ∏ s (w1 * dt)
  let s2 := leapfrogStep L Œ∏ s1 (w0 * dt)
  leapfrogStep L Œ∏ s2 (w1 * dt)

/--
  **Noisy Hamiltonian**
  Structure representing a Hamiltonian system subject to dissipation (friction)
  and fluctuation (thermal noise), governing Underdamped Langevin Dynamics.
-/
structure NoisyHamiltonian (P Q M : Type*)
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup Q] [InnerProductSpace ‚Ñù Q] [CompleteSpace Q]
  [NormedAddCommGroup M] [InnerProductSpace ‚Ñù M] [CompleteSpace M]
  extends HamiltonianLens P Q M where

  /-- Friction coefficient Œ≥ (dissipation) -/
  friction : ‚Ñù

  /-- Temperature T (fluctuation) -/
  temperature : ‚Ñù

/--
  **Stochastic Gradient Langevin Dynamics (SGLD)**
  An update rule for large-scale Bayesian learning that combines stochastic gradients
  with Langevin dynamics to sample from the posterior distribution of parameters.
  Œ∏_{t+1} = Œ∏_t - (Œµ/2)‚àá_Œ∏ E(Œ∏_t, s) + ‚àöŒµ Œæ
-/
noncomputable def StochasticGradientLangevinDynamics {P S : Type*}
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup S] [InnerProductSpace ‚Ñù S] [CompleteSpace S]
  (L : EnergyLens P S) (Œ∏ : P) (batch_data : S) (noise : P) (epsilon : ‚Ñù) : P :=
  let grad_Œ∏ := (L.force_vector Œ∏ batch_data).1
  Œ∏ - ((epsilon / 2) ‚Ä¢ grad_Œ∏) + (Real.sqrt epsilon ‚Ä¢ noise)

/--
  **BAOAB Integrator**
  A high-accuracy splitting method for Underdamped Langevin Dynamics.
  Splits the evolution into:
  - B: Deterministic momentum update (Kick)
  - A: Deterministic position update (Drift)
  - O: Ornstein-Uhlenbeck noise/friction process
  - A: Deterministic position update (Drift)
  - B: Deterministic momentum update (Kick)
-/
noncomputable def baoabIntegrator {P Q M : Type*}
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup Q] [InnerProductSpace ‚Ñù Q] [CompleteSpace Q]
  [NormedAddCommGroup M] [InnerProductSpace ‚Ñù M] [CompleteSpace M]
  (L : NoisyHamiltonian P Q M) (Œ∏ : P) (s : Q √ó M) (noise : M) (dt : ‚Ñù) : Q √ó M :=
  let (q, m) := s
  let Œ≥ := L.friction
  let T := L.temperature

  -- 1. B: Half-step Momentum Kick
  let grad_q_1 := (L.force_vector Œ∏ (q, m)).2.1
  let m_1 := m - (dt / 2) ‚Ä¢ grad_q_1

  -- 2. A: Half-step Position Drift
  let grad_m_1 := (L.force_vector Œ∏ (q, m_1)).2.2
  let q_1 := q + (dt / 2) ‚Ä¢ grad_m_1

  -- 3. O: Ornstein-Uhlenbeck Step (Exact)
  let c1 := Real.exp (-Œ≥ * dt)
  let c2 := Real.sqrt (T * (1 - c1 ^ 2))
  let m_2 := c1 ‚Ä¢ m_1 + c2 ‚Ä¢ noise

  -- 4. A: Half-step Position Drift
  let grad_m_2 := (L.force_vector Œ∏ (q_1, m_2)).2.2
  let q_2 := q_1 + (dt / 2) ‚Ä¢ grad_m_2

  -- 5. B: Half-step Momentum Kick
  let grad_q_2 := (L.force_vector Œ∏ (q_2, m_2)).2.1
  let m_3 := m_2 - (dt / 2) ‚Ä¢ grad_q_2

  (q_2, m_3)

/--
  **Bayesian Neural Network**
  Wraps an EnergyLens (defining the posterior landscape) and provides
  probabilistic inference via Stochastic Gradient Langevin Dynamics.
-/
structure BayesianNeuralNetwork (P S : Type*)
  [NormedAddCommGroup P] [InnerProductSpace ‚Ñù P] [CompleteSpace P]
  [NormedAddCommGroup S] [InnerProductSpace ‚Ñù S] [CompleteSpace S] where

  /-- The potential energy function (Negative Log Posterior) -/
  model : EnergyLens P S

  /-- Step size for the SGLD integrator -/
  learning_rate : ‚Ñù

  /-- Perform one step of Bayesian inference -/
  infer : P ‚Üí S ‚Üí P ‚Üí P :=
    fun Œ∏ batch noise => StochasticGradientLangevinDynamics model Œ∏ batch noise learning_rate

/-! ### 4. Composition and Transformers -/

section Transformer

variable {ùïú : Type*} [NontriviallyNormedField ùïú]

/-- Composition of two computational blocks.
    If B1 : X ‚Üí Y and B2 : Y ‚Üí Z, then B2 ‚àò B1 : X ‚Üí Z.
    Parameters are paired: (P1 √ó P2).
-/
def CompBlock.compose {P1 P2 X Y Z : Type*}
    [NormedAddCommGroup P1] [NormedSpace ùïú P1]
    [NormedAddCommGroup P2] [NormedSpace ùïú P2]
    [NormedAddCommGroup X] [NormedSpace ùïú X]
    [NormedAddCommGroup Y] [NormedSpace ùïú Y]
    [NormedAddCommGroup Z] [NormedSpace ùïú Z]
    (B2 : CompBlock ùïú P2 Y Z) (B1 : CompBlock ùïú P1 X Y) :
    CompBlock ùïú (P1 √ó P2) X Z where
  fwd := fun p_x => B2.fwd (p_x.1.2, B1.fwd (p_x.1.1, p_x.2))
  diff := by
    apply Differentiable.comp B2.diff
    apply Differentiable.prod
    ¬∑ exact Differentiable.comp differentiable_snd differentiable_fst
    ¬∑ apply Differentiable.comp B1.diff
      apply Differentiable.prod
      ¬∑ exact Differentiable.comp differentiable_fst differentiable_fst
      ¬∑ exact differentiable_snd

/-- Residual connection: x ‚Ü¶ x + B(x).
    Requires input and output spaces to be the same.
-/
def CompBlock.residual {P X : Type*}
    [NormedAddCommGroup P] [NormedSpace ùïú P]
    [NormedAddCommGroup X] [NormedSpace ùïú X]
    (B : CompBlock ùïú P X X) : CompBlock ùïú P X X where
  fwd := fun p_x => p_x.2 + B.fwd p_x
  diff := Differentiable.add differentiable_snd B.diff

variable {D_model : Type*} [NormedAddCommGroup D_model] [NormedSpace ùïú D_model]
variable {P_Attn P_FFN : Type*}
  [NormedAddCommGroup P_Attn] [NormedSpace ùïú P_Attn]
  [NormedAddCommGroup P_FFN] [NormedSpace ùïú P_FFN]
variable {P_LN1 P_LN2 : Type*}
  [NormedAddCommGroup P_LN1] [NormedSpace ùïú P_LN1]
  [NormedAddCommGroup P_LN2] [NormedSpace ùïú P_LN2]

/--
  A Transformer Block consists of:
  1. A Self-Attention layer (with residual connection) followed by LayerNorm.
  2. A Feed-Forward Network (with residual connection) followed by LayerNorm.
-/
def TransformerBlock
    (Attention : CompBlock ùïú P_Attn D_model D_model)
    (FFN : CompBlock ùïú P_FFN D_model D_model)
    (LN1 : CompBlock ùïú P_LN1 D_model D_model)
    (LN2 : CompBlock ùïú P_LN2 D_model D_model) :
    CompBlock ùïú ((P_Attn √ó P_LN1) √ó (P_FFN √ó P_LN2)) D_model D_model :=
  (LN2.compose FFN.residual).compose (LN1.compose Attention.residual)

end Transformer

/-! ### 5. Linear Layers and Attention -/

section LinearLayers

variable {n m : Type*} [Fintype n] [Fintype m] [DecidableEq n] [DecidableEq m]

/-- Reshapes a flattened Euclidean vector into a Matrix (general dimensions) -/
noncomputable def toMatrixGen (v : EuclideanSpace ‚Ñù (m √ó n)) : Matrix m n ‚Ñù :=
  Matrix.of (fun i j => v (i, j))

/--
  Linear Layer: f(x) = Wx + b
  Parameters: (Weights, Bias)
-/
noncomputable def Linear : CompBlock ‚Ñù (EuclideanSpace ‚Ñù (m √ó n) √ó EuclideanSpace ‚Ñù m)
    (EuclideanSpace ‚Ñù n) (EuclideanSpace ‚Ñù m) where
  fwd := fun ((w_flat, b), x) =>
    let W := toMatrixGen w_flat
    toEuclideanFun (W.mulVec x) + b
  diff := sorry -- Linear maps are differentiable

/-- Example: Feed-Forward Network using Linear blocks -/
def FFN (d_model d_hidden : Type*) [Fintype d_model] [Fintype d_hidden] [DecidableEq d_model] [DecidableEq d_hidden]
    (Activation : CompBlock ‚Ñù Unit (EuclideanSpace ‚Ñù d_hidden) (EuclideanSpace ‚Ñù d_hidden)) :
    CompBlock ‚Ñù _ (EuclideanSpace ‚Ñù d_model) (EuclideanSpace ‚Ñù d_model) :=
  let L1 := Linear (n := d_model) (m := d_hidden)
  let L2 := Linear (n := d_hidden) (m := d_model)
  L2.compose (Activation.compose L1)

/-- Layer Normalization: y = (x - Œº) / ‚àö(œÉ¬≤ + Œµ) * Œ≥ + Œ≤ -/
noncomputable def LayerNorm (Œµ : ‚Ñù) : CompBlock ‚Ñù (EuclideanSpace ‚Ñù n √ó EuclideanSpace ‚Ñù n) (EuclideanSpace ‚Ñù n) (EuclideanSpace ‚Ñù n) where
  fwd := fun ((Œ≥, Œ≤), x) =>
    let card := Fintype.card n
    let Œº := (‚àë i, x i) / card
    let var := (‚àë i, (x i - Œº)^2) / card
    let x_hat := toEuclideanFun (fun i => (x i - Œº) / Real.sqrt (var + Œµ))
    toEuclideanFun (fun i => Œ≥ i * x_hat i + Œ≤ i)
  diff := sorry

end LinearLayers

section Attention

variable {d_model d_head : Type*} [Fintype d_model] [Fintype d_head]
         [DecidableEq d_model] [DecidableEq d_head]

/-- Softmax activation (parameter-less) -/
def Softmax : CompBlock ‚Ñù Unit (EuclideanSpace ‚Ñù d_head) (EuclideanSpace ‚Ñù d_head) where
  fwd := fun (_, x) =>
    let exp_x := toEuclideanFun (fun i => Real.exp (x i))
    let sum_exp := ‚àë i, exp_x i
    (sum_exp‚Åª¬π) ‚Ä¢ exp_x
  diff := sorry

/-- Parallel composition (Fan-out): Passes input to both blocks. -/
def CompBlock.fanout {P1 P2 X Y1 Y2 : Type*}
    [NormedAddCommGroup P1] [NormedSpace ‚Ñù P1] [NormedAddCommGroup P2] [NormedSpace ‚Ñù P2]
    [NormedAddCommGroup X] [NormedSpace ‚Ñù X]
    [NormedAddCommGroup Y1] [NormedSpace ‚Ñù Y1] [NormedAddCommGroup Y2] [NormedSpace ‚Ñù Y2]
    (B1 : CompBlock ‚Ñù P1 X Y1) (B2 : CompBlock ‚Ñù P2 X Y2) :
    CompBlock ‚Ñù (P1 √ó P2) X (Y1 √ó Y2) where
  fwd := fun ((p1, p2), x) => (B1.fwd (p1, x), B2.fwd (p2, x))
  diff := sorry

/--
  Attention Mechanism: Softmax(Q * K) * V
  Note: We use element-wise multiplication for Q and K to produce a vector for Softmax,
  interpreting the "dot product" requirement as a feature-wise interaction in this type context.
-/
def AttentionMechanism : CompBlock ‚Ñù Unit ((EuclideanSpace ‚Ñù d_head √ó EuclideanSpace ‚Ñù d_head) √ó EuclideanSpace ‚Ñù d_head) (EuclideanSpace ‚Ñù d_head) where
  fwd := fun ((q, k), v) =>
    let scores := toEuclideanFun (fun i => q i * k i)
    let weights := Softmax.fwd ((), scores)
    toEuclideanFun (fun i => weights i * v i)
  diff := sorry

/--
  Multi-Head Attention constructed by composing Linear blocks.
  We assume an `AttentionMechanism` block is provided (which uses Softmax internally).
-/
def MultiHeadAttention
    (AttentionMechanism : CompBlock ‚Ñù Unit ((EuclideanSpace ‚Ñù d_head √ó EuclideanSpace ‚Ñù d_head) √ó EuclideanSpace ‚Ñù d_head) (EuclideanSpace ‚Ñù d_head)) :
    CompBlock ‚Ñù _ (EuclideanSpace ‚Ñù d_model) (EuclideanSpace ‚Ñù d_model) :=
  let W_Q := Linear (n := d_model) (m := d_head)
  let W_K := Linear (n := d_model) (m := d_head)
  let W_V := Linear (n := d_model) (m := d_head)
  let W_O := Linear (n := d_head)  (m := d_model)
  W_O.compose (AttentionMechanism.compose ((W_Q.fanout W_K).fanout W_V))

end Attention

/-! ### 6. Convolutional Layers -/

section Convolution

variable {C_in C_out : Type*} [Fintype C_in] [Fintype C_out] [DecidableEq C_in] [DecidableEq C_out]

/--
  2D Convolution with valid padding.
  Input: (C_in, H, W)
  Output: (C_out, H - K + 1, W - K + 1)
  Kernel: (C_out, C_in, K, K)
  Parameters: (Weights, Bias)
-/
noncomputable def Conv2D (H W K : ‚Ñï) (hH : K ‚â§ H) (hW : K ‚â§ W) :
    CompBlock ‚Ñù
      (EuclideanSpace ‚Ñù (C_out √ó C_in √ó Fin K √ó Fin K) √ó EuclideanSpace ‚Ñù C_out)
      (EuclideanSpace ‚Ñù (C_in √ó Fin H √ó Fin W))
      (EuclideanSpace ‚Ñù (C_out √ó Fin (H - K + 1) √ó Fin (W - K + 1))) where
  fwd := fun ((weights, bias), x) =>
    toEuclideanFun fun (co, h, w) =>
      let val := ‚àë ci, ‚àë kh, ‚àë kw,
        weights (co, ci, kh, kw) * x (ci,
          ‚ü®h.val + kh.val, by have := h.isLt; have := kh.isLt; omega‚ü©,
          ‚ü®w.val + kw.val, by have := w.isLt; have := kw.isLt; omega‚ü©)
      val + bias co
  diff := sorry

end Convolution

--#min_imports
